\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with`a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx,float}
\usepackage{xcolor}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Assignment 3}
\author{Miao-Chin Yen}

\begin{document}
\maketitle

\section*{Problem 1}
\hspace{1em}For a deterministic Policy, $\pi_{D}: \mathcal{S}\rightarrow \mathcal{A}$, i.e., $\pi_{D}(s) = a$, where $s\in \mathcal{S}, a\in\mathcal{A}$.\\
MDP(State-Value Function) Bellman Policy Equation $V^{\pi_{D}}: \mathcal{N} \rightarrow \mathbb{R}$:
$$
V^{\pi_{D}}(s)=\mathcal{R}(s, \pi_{D}(s))+\gamma \cdot \sum_{s^{\prime} \in \mathcal{N}} \mathcal{P}\left(s,\pi_{D}(s) , s^{\prime}\right) \cdot V^{\pi_{D}}\left(s^{\prime}\right)
$$
Action-Value Function (for policy $\pi_{D}$) $Q^{\pi_{D}}: \mathcal{N} \times \mathcal{A} \rightarrow \mathbb{R}$:
$$Q^{\pi_{D}}(s, \pi_{D}(s))=\mathcal{R}(s, \pi_{D}(s))+\gamma \cdot \sum_{s^{\prime} \in \mathcal{N}} \mathcal{P}\left(s,\pi_{D}(s) , s^{\prime}\right) \cdot V^{\pi_{D}}\left(s^{\prime}\right)
$$
$$
V^{\pi_{D}}(s)=Q^{\pi_{D}}(s, \pi_{D}(s))
$$

$$
Q^{\pi_{D}}(s, \pi_{D}(s))=\mathcal{R}(s, \pi_{D}(s))+\gamma \cdot \sum_{s^{\prime} \in \mathcal{N}} \mathcal{P}\left(s, \pi_{D}(s), s^{\prime}\right)   \cdot Q^{\pi_{D}}\left(s^{\prime}, \pi_{D}(s^{\prime})\right)
$$

\section*{Problem 2}
MDP State-Value Function Bellman Optimality Equation:
$$
V^{*}(s)=\max _{a \in \mathcal{A}}\left\{\mathcal{R}(s, a)+\gamma \cdot \sum_{s^{\prime} \in \mathcal{N}} \mathcal{P}\left(s, a, s^{\prime}\right) \cdot V^{*}\left(s^{\prime}\right)\right\}
$$
In this problem, $\mathcal{A}=[0,1 ]$ and $\gamma = 0.5$:
$$
V^{*}(s)=\max _{a \in [0,1]}\left\{\mathcal{R}(s, a)+ 0.5 \cdot \sum_{s^{\prime} \in \mathcal{N}} \mathcal{P}\left(s, a, s^{\prime}\right) \cdot V^{*}\left(s^{\prime}\right)\right\}
$$
and we write explicitly:
$$
V^{*}(s)=\max _{a \in [0,1]}\left\{a(1-a)+(1-a)(1+a) + 0.5 \cdot[V^{*}(s+1)\cdot a + V^{*}(s)\cdot (1-a)] \right\}
$$
Notice that $\mathcal{R}(s, a)$ does not depend on $s$. Hence, $V^{*}(s) = V^{*}(s+1)$. Therefore,
$$
V^{*}(s)=\max _{a \in [0,1]}\left\{a(1-a)+(1-a)(1+a) + 0.5 \cdot V^{*}(s+1) \right\}  \Longrightarrow a = 0.25
$$
$$
V^{*}(s)=1.125+ 0.5 \cdot V^{*}(s+1) 
$$
and the optimal deterministic policy $\pi_{D}(s)= 0.25 \text{ }\forall s \in \mathcal{S}$

\section*{Problem 3}
\textcolor{blue}{TBD}

\section*{Problem 4}
MDP State-Value Function Bellman Optimality Equation:
$$
V^{*}(s)=\max _{a \in \mathcal{A}}\left\{\mathcal{R}(s, a)+\gamma \cdot \sum_{s^{\prime} \in \mathcal{N}} \mathcal{P}\left(s, a, s^{\prime}\right) \cdot V^{*}\left(s^{\prime}\right)\right\}
$$
Consider the myopic case ($\gamma = 0$) and $S^{\prime}\sim \mathcal{N}(s,\,\sigma^{2})$:
$$
V^{*}(s)=\max _{a \in \mathcal{A}}\mathcal{R}(s, a) = \max _{a \in \mathcal{A}}\sum_{s^{\prime} \in \mathbb{R}} \mathcal{P}_{S^{\prime}}\left( s^{\prime}\right)\cdot [-e ^{as^{\prime}}] = \max _{a \in \mathcal{A}}\mathbb{E}[-e^{as^{\prime}}] = \min _{a \in \mathcal{A}} M_{S^{\prime}}(a)
$$
$$
 M_{S^{\prime}}(a)=e^{sa+\frac{\sigma^2 a^2}{2}}
$$
To find $a$ which maximizes the moment generating function, we take derivative w.r.t. $a$.\\
$$
 e^{sa+\frac{\sigma^2 a^2}{2}}\cdot (s + \sigma^2 a) = 0 \Longrightarrow s + \sigma^2 a = 0 \Longrightarrow a = \frac{-s}{\sigma^2}
$$\\
Hence, the optimal action $a^{*}$ for state $s$ is $\frac{-s}{\sigma^2}$ and the corresponding optimal cost is $V^{*}(s) = e^{\frac{-s^2}{\sigma^2} + \frac{s^2}{2\sigma^2}}$






\end{document}