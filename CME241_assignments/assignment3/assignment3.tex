\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with`a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx,float}
\usepackage{xcolor}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Assignment 3}
\author{Miao-Chin Yen}

\begin{document}
\maketitle

\section*{Problem 1}
\hspace{1em}For a deterministic Policy, $\pi_{D}: \mathcal{S}\rightarrow \mathcal{A}$, i.e., $\pi_{D}(s) = a$, where $s\in \mathcal{S}, a\in\mathcal{A}$.\\
MDP(State-Value Function) Bellman Policy Equation $V^{\pi_{D}}: \mathcal{N} \rightarrow \mathbb{R}$:
$$
V^{\pi_{D}}(s)=\mathcal{R}(s, \pi_{D}(s))+\gamma \cdot \sum_{s^{\prime} \in \mathcal{N}} \mathcal{P}\left(s,\pi_{D}(s) , s^{\prime}\right) \cdot V^{\pi_{D}}\left(s^{\prime}\right)
$$
Action-Value Function (for policy $\pi_{D}$) $Q^{\pi_{D}}: \mathcal{N} \times \mathcal{A} \rightarrow \mathbb{R}$:
$$Q^{\pi_{D}}(s, \pi_{D}(s))=\mathcal{R}(s, \pi_{D}(s))+\gamma \cdot \sum_{s^{\prime} \in \mathcal{N}} \mathcal{P}\left(s,\pi_{D}(s) , s^{\prime}\right) \cdot V^{\pi_{D}}\left(s^{\prime}\right)
$$
$$
V^{\pi_{D}}(s)=Q^{\pi_{D}}(s, \pi_{D}(s))
$$

$$
Q^{\pi_{D}}(s, \pi_{D}(s))=\mathcal{R}(s, \pi_{D}(s))+\gamma \cdot \sum_{s^{\prime} \in \mathcal{N}} \mathcal{P}\left(s, \pi_{D}(s), s^{\prime}\right)   \cdot Q^{\pi_{D}}\left(s^{\prime}, \pi_{D}(s^{\prime})\right)
$$

\section*{Problem 2}
MDP State-Value Function Bellman Optimality Equation:
$$
V^{*}(s)=\max _{a \in \mathcal{A}}\left\{\mathcal{R}(s, a)+\gamma \cdot \sum_{s^{\prime} \in \mathcal{N}} \mathcal{P}\left(s, a, s^{\prime}\right) \cdot V^{*}\left(s^{\prime}\right)\right\}
$$
In this problem, $\mathcal{A}=[0,1 ]$ and $\gamma = 0.5$:
$$
V^{*}(s)=\max _{a \in [0,1]}\left\{\mathcal{R}(s, a)+ 0.5 \cdot \sum_{s^{\prime} \in \mathcal{N}} \mathcal{P}\left(s, a, s^{\prime}\right) \cdot V^{*}\left(s^{\prime}\right)\right\}
$$
and we write explicitly:
$$
V^{*}(s)=\max _{a \in [0,1]}\left\{a(1-a)+(1-a)(1+a) + 0.5 \cdot[V^{*}(s+1)\cdot a + V^{*}(s)\cdot (1-a)] \right\}
$$
Notice that $\mathcal{R}(s, a)$ does not depend on $s$. Hence, $V^{*}(s) = V^{*}(s+1)$. Therefore,
$$
V^{*}(s)=\max _{a \in [0,1]}\left\{a(1-a)+(1-a)(1+a) + 0.5 \cdot V^{*}(s+1) \right\}  \Longrightarrow a = 0.25
$$
$$
V^{*}(s)=1.125+ 0.5 \cdot V^{*}(s+1) 
$$
and the optimal deterministic policy $\pi_{D}(s)= 0.25 \text{ }\forall s \in \mathcal{S}$

\section*{Problem 3}
Let the state space $\mathcal{S}=\{s \mid 0 \leq s \leq n\}$. State $s$ means that the frog is sitting on lilypad numbered $s$. Terminal state space: $\mathcal{T}=\{0, n\}.$ The action space $\mathcal{A}=\{A, B\}$ which stands for the two choices of croak sounds. The state transitions are as follows:
$$
\begin{gathered}
\mathcal{P}(s, A, s^{\prime}) = \mathbb{P}[S_{t+1} = s^{\prime}| S_{t} = s, A_{t} = A] \text { for } 1 \leq s \leq n-1= \begin{cases}\frac{s}{n} & \text { for } s^{\prime}=s-1 \\
\frac{n-s}{n} & \text { for } s^{\prime}=s+1 \\
0 & \text { otherwise }\end{cases} \\
\mathcal{P}(s, B, s^{\prime}) =  \mathbb{P}[S_{t+1} = s^{\prime}| S_{t} = s, A_{t} = B] \text { for } 1 \leq s \leq n-1= \begin{cases}\frac{1}{n} & \text { for all } 0 \leq s^{\prime} \leq n \text { and } s^{\prime} \neq s \\
0 & \text { for } s^{\prime}=s\end{cases}
\end{gathered}
$$
The reward function $R\left(s, a, s^{\prime}\right)$ is as follows:
$$
R\left(s, a, s^{\prime}\right) \text { for } 1 \leq s \leq n-1, a \in\{A, B\}= \begin{cases}1 & \text { for } s^{\prime}=n \\ 0 & \text { otherwise }\end{cases}
$$
Let discount factor to be 0.9. We have the following graphs.\\
n=3:
\begin{center}
\includegraphics[scale=0.3]{lilypadnumber_3}
\end{center}
n=6:
\begin{center}
\includegraphics[scale=0.3]{lilypadnumber_6}
\end{center}
n=9:
\begin{center}
\includegraphics[scale=0.3]{lilypadnumber_9}
\end{center}
We found that for $1\leq s\leq n-2$, the frog should croak B. For $s=n-1$, it should croak A.\\
Reference \href{https://github.com/miaochin/RL-book/tree/master/CME241_assignments/assignment3}{\textcolor{blue}{croaking\textunderscore on\textunderscore lilypads\textunderscore mdp.py}}.
\section*{Problem 4}
MDP State-Value Function Bellman Optimality Equation:
$$
V^{*}(s)=\max _{a \in \mathcal{A}}\left\{\mathcal{R}(s, a)+\gamma \cdot \sum_{s^{\prime} \in \mathcal{N}} \mathcal{P}\left(s, a, s^{\prime}\right) \cdot V^{*}\left(s^{\prime}\right)\right\}
$$
Consider the myopic case ($\gamma = 0$) and $S^{\prime}\sim \mathcal{N}(s,\,\sigma^{2})$:
$$
V^{*}(s)=\max _{a \in \mathcal{A}}\mathcal{R}(s, a) = \max _{a \in \mathcal{A}}\sum_{s^{\prime} \in \mathbb{R}} \mathcal{P}_{S^{\prime}}\left( s^{\prime}\right)\cdot [-e ^{as^{\prime}}] = \max _{a \in \mathcal{A}}\mathbb{E}[-e^{as^{\prime}}] = \min _{a \in \mathcal{A}} M_{S^{\prime}}(a)
$$
$$
 M_{S^{\prime}}(a)=e^{sa+\frac{\sigma^2 a^2}{2}}
$$
To find $a$ which maximizes the moment generating function, we take derivative w.r.t. $a$.\\
$$
 e^{sa+\frac{\sigma^2 a^2}{2}}\cdot (s + \sigma^2 a) = 0 \Longrightarrow s + \sigma^2 a = 0 \Longrightarrow a = \frac{-s}{\sigma^2}
$$\\
Hence, the optimal action $a^{*}$ for state $s$ is $\frac{-s}{\sigma^2}$ and the corresponding optimal cost is $V^{*}(s) = e^{\frac{-s^2}{\sigma^2} + \frac{s^2}{2\sigma^2}}$






\end{document}